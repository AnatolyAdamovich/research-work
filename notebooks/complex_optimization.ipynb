{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# change the current working directory\n",
    "os.chdir('..')\n",
    "\n",
    "# main\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "# implementation\n",
    "from tools import make_regression_data, RegressionDataset, finite_time_opt_training, drem_opt_training,\\\n",
    "                                        standard_training, plot_results, validation_epoch\n",
    "from optimizers import FiniteTimeOptimizer, DREMOptimizer\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.random.manual_seed(19)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "We will work with simple regression data with high noise level"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (torch.Size([800, 10]), torch.Size([800, 1]))\n",
      "shape of test: (torch.Size([200, 10]), torch.Size([200, 1]))\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_FEATURES = 10\n",
    "X_train, X_test, y_train, y_test = make_regression_data(number_samples=1000,\n",
    "                                                        number_features=NUMBER_OF_FEATURES,\n",
    "                                                        noise_value=0.5)\n",
    "print(f'shape of train: {X_train.shape, y_train.shape}\\nshape of test: {X_test.shape, y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (torch.Size([800, 10]), torch.Size([800, 1]))\n",
      "shape of test: (torch.Size([200, 10]), torch.Size([200, 1]))\n",
      "example of train sample:\n",
      " (tensor([ 0.5686,  0.6552,  1.6264,  0.3829, -1.3236,  0.2570,  0.2360, -0.4359,\n",
      "         0.3517,  0.8775]), tensor([176.7505]))\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of train: {X_train.shape, y_train.shape}\\nshape of test: {X_test.shape, y_test.shape}')\n",
    "train_dataset = RegressionDataset(features=X_train,\n",
    "                                  labels=y_train)\n",
    "test_dataset = RegressionDataset(features=X_test,\n",
    "                                 labels=y_test)\n",
    "print(f'example of train sample:\\n {train_dataset[21]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch: features - torch.Size([10, 10]) and labels - torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "batch_example_features, batch_example_labels  = next(iter(train_dataloader))\n",
    "print('shape of batch: features - {} and labels - {}'.format(batch_example_features.shape, batch_example_labels.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and score function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "metric_fn = R2Score()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.fc_first = nn.Linear(10, 15)\n",
    "        self.fc_main = nn.Sequential(nn.Linear(15, 25),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(25, 1),\n",
    "                                     nn.ReLU())\n",
    "        # self.fc_last = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_first(x)\n",
    "        x = self.fc_main(x)\n",
    "        # x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "model = ComplexNet().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "N_OF_BATCHES = 5\n",
    "\n",
    "# optimizer = FiniteTimeOptimizer(params=model.fc_last.parameters(),\n",
    "#                                 lr=0.001,\n",
    "#                                 n_of_batches=N_OF_BATCHES)\n",
    "\n",
    "# optimizer2 = SGD(params=model.fc_last.parameters(),\n",
    "#                  lr=0.01)\n",
    "#\n",
    "# optimizer3 = Adam(params=model.fc_main.parameters(),\n",
    "#                   lr=0.0001)\n",
    "\n",
    "optimizer_sgd = SGD(params=model.fc_main.parameters(), lr=0.01)\n",
    "\n",
    "optimizer_drem = DREMOptimizer(params=model.fc_first.parameters(),\n",
    "                               lr=1e-5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the 1th epoch: loss = nan & score = nan\n",
      "on the 2th epoch: loss = nan & score = nan\n",
      "on the 3th epoch: loss = nan & score = nan\n",
      "on the 4th epoch: loss = nan & score = nan\n",
      "on the 5th epoch: loss = nan & score = nan\n",
      "on the 6th epoch: loss = nan & score = nan\n",
      "on the 7th epoch: loss = nan & score = nan\n",
      "on the 8th epoch: loss = nan & score = nan\n",
      "on the 9th epoch: loss = nan & score = nan\n",
      "on the 10th epoch: loss = nan & score = nan\n",
      "on the 11th epoch: loss = nan & score = nan\n",
      "on the 12th epoch: loss = nan & score = nan\n",
      "on the 13th epoch: loss = nan & score = nan\n",
      "on the 14th epoch: loss = nan & score = nan\n",
      "on the 15th epoch: loss = nan & score = nan\n",
      "on the 16th epoch: loss = nan & score = nan\n",
      "on the 17th epoch: loss = nan & score = nan\n",
      "on the 18th epoch: loss = nan & score = nan\n",
      "on the 19th epoch: loss = nan & score = nan\n",
      "on the 20th epoch: loss = nan & score = nan\n",
      "on the 21th epoch: loss = nan & score = nan\n",
      "on the 22th epoch: loss = nan & score = nan\n",
      "on the 23th epoch: loss = nan & score = nan\n",
      "on the 24th epoch: loss = nan & score = nan\n",
      "on the 25th epoch: loss = nan & score = nan\n",
      "on the 26th epoch: loss = nan & score = nan\n",
      "on the 27th epoch: loss = nan & score = nan\n",
      "on the 28th epoch: loss = nan & score = nan\n",
      "on the 29th epoch: loss = nan & score = nan\n",
      "on the 30th epoch: loss = nan & score = nan\n",
      "on the 31th epoch: loss = nan & score = nan\n",
      "on the 32th epoch: loss = nan & score = nan\n",
      "on the 33th epoch: loss = nan & score = nan\n",
      "on the 34th epoch: loss = nan & score = nan\n",
      "on the 35th epoch: loss = nan & score = nan\n",
      "on the 36th epoch: loss = nan & score = nan\n",
      "on the 37th epoch: loss = nan & score = nan\n",
      "on the 38th epoch: loss = nan & score = nan\n",
      "on the 39th epoch: loss = nan & score = nan\n",
      "on the 40th epoch: loss = nan & score = nan\n",
      "on the 41th epoch: loss = nan & score = nan\n",
      "on the 42th epoch: loss = nan & score = nan\n",
      "on the 43th epoch: loss = nan & score = nan\n",
      "on the 44th epoch: loss = nan & score = nan\n",
      "on the 45th epoch: loss = nan & score = nan\n",
      "on the 46th epoch: loss = nan & score = nan\n",
      "on the 47th epoch: loss = nan & score = nan\n",
      "on the 48th epoch: loss = nan & score = nan\n",
      "on the 49th epoch: loss = nan & score = nan\n",
      "on the 50th epoch: loss = nan & score = nan\n",
      "on the 51th epoch: loss = nan & score = nan\n",
      "on the 52th epoch: loss = nan & score = nan\n",
      "on the 53th epoch: loss = nan & score = nan\n",
      "on the 54th epoch: loss = nan & score = nan\n",
      "on the 55th epoch: loss = nan & score = nan\n",
      "on the 56th epoch: loss = nan & score = nan\n",
      "on the 57th epoch: loss = nan & score = nan\n",
      "on the 58th epoch: loss = nan & score = nan\n",
      "on the 59th epoch: loss = nan & score = nan\n",
      "on the 60th epoch: loss = nan & score = nan\n",
      "on the 61th epoch: loss = nan & score = nan\n",
      "on the 62th epoch: loss = nan & score = nan\n",
      "on the 63th epoch: loss = nan & score = nan\n",
      "on the 64th epoch: loss = nan & score = nan\n",
      "on the 65th epoch: loss = nan & score = nan\n",
      "on the 66th epoch: loss = nan & score = nan\n",
      "on the 67th epoch: loss = nan & score = nan\n",
      "on the 68th epoch: loss = nan & score = nan\n",
      "on the 69th epoch: loss = nan & score = nan\n",
      "on the 70th epoch: loss = nan & score = nan\n",
      "on the 71th epoch: loss = nan & score = nan\n",
      "on the 72th epoch: loss = nan & score = nan\n",
      "on the 73th epoch: loss = nan & score = nan\n",
      "on the 74th epoch: loss = nan & score = nan\n",
      "on the 75th epoch: loss = nan & score = nan\n",
      "on the 76th epoch: loss = nan & score = nan\n",
      "on the 77th epoch: loss = nan & score = nan\n",
      "on the 78th epoch: loss = nan & score = nan\n",
      "on the 79th epoch: loss = nan & score = nan\n",
      "on the 80th epoch: loss = nan & score = nan\n",
      "on the 81th epoch: loss = nan & score = nan\n",
      "on the 82th epoch: loss = nan & score = nan\n",
      "on the 83th epoch: loss = nan & score = nan\n",
      "on the 84th epoch: loss = nan & score = nan\n",
      "on the 85th epoch: loss = nan & score = nan\n",
      "on the 86th epoch: loss = nan & score = nan\n",
      "on the 87th epoch: loss = nan & score = nan\n",
      "on the 88th epoch: loss = nan & score = nan\n",
      "on the 89th epoch: loss = nan & score = nan\n",
      "on the 90th epoch: loss = nan & score = nan\n",
      "on the 91th epoch: loss = nan & score = nan\n",
      "on the 92th epoch: loss = nan & score = nan\n",
      "on the 93th epoch: loss = nan & score = nan\n",
      "on the 94th epoch: loss = nan & score = nan\n",
      "on the 95th epoch: loss = nan & score = nan\n",
      "on the 96th epoch: loss = nan & score = nan\n",
      "on the 97th epoch: loss = nan & score = nan\n",
      "on the 98th epoch: loss = nan & score = nan\n",
      "on the 99th epoch: loss = nan & score = nan\n",
      "on the 100th epoch: loss = nan & score = nan\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "new_loss = True\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # train\n",
    "    loss_epoch = 0.0\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        predicted = model(X_batch)\n",
    "        if new_loss:\n",
    "            determinant = torch.det(X_batch)\n",
    "            inverse_batch = torch.linalg.inv(X_batch)\n",
    "            adjoin = determinant * inverse_batch\n",
    "            loss = loss_fn(adjoin @ predicted, adjoin @ y_batch)\n",
    "        else:\n",
    "            loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        loss_epoch += loss.detach()\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer_drem.zero_grad()\n",
    "        optimizer_sgd.zero_grad()\n",
    "        # optimizer3.zero_grad()\n",
    "\n",
    "        # backpropagation (compute gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        # optimizer3.step()\n",
    "        optimizer_drem.step(det_batch=determinant)\n",
    "        optimizer_sgd.step()\n",
    "\n",
    "    # evaluate\n",
    "    mean_loss_test, mean_metric_test = validation_epoch(model, loss_fn, metric_fn, test_dataloader, \"cpu\")\n",
    "\n",
    "    print(f'on the {epoch}th epoch: loss = {(loss_epoch) / (len(train_dataloader)):.3f} & score = {mean_metric_test}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], grad_fn=<ReluBackward0>)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_epoch = 0.0\n",
    "    model.train()\n",
    "    for X_batch, y_batch in data_train:\n",
    "        X_batch, y_batch = X_batch.to(current_device), y_batch.to(current_device)\n",
    "        # forward pass\n",
    "        predicted = model(X_batch)\n",
    "        if new_loss:\n",
    "            determinant = torch.det(X_batch)\n",
    "            inverse_batch = torch.linalg.inv(X_batch)\n",
    "            adjoin = determinant * inverse_batch\n",
    "            loss = loss_fn(adjoin @ predicted, adjoin @ y_batch)\n",
    "        else:\n",
    "            loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        loss_epoch += loss.detach()\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backpropagation (compute gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
