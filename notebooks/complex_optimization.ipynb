{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# change the current working directory\n",
    "os.chdir('..')\n",
    "\n",
    "# main\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import R2Score\n",
    "import numpy as np\n",
    "\n",
    "# implementation\n",
    "from tools import make_regression_data, RegressionDataset, finite_time_opt_training, drem_opt_training,\\\n",
    "                                        standard_training, plot_results, validation_epoch\n",
    "from optimizers import FiniteTimeOptimizer, DREMOptimizer\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.random.manual_seed(19)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "We will work with simple regression data with high noise level"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (torch.Size([800, 10]), torch.Size([800, 1]))\n",
      "shape of test: (torch.Size([200, 10]), torch.Size([200, 1]))\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_FEATURES = 10\n",
    "X_train, X_test, y_train, y_test = make_regression_data(number_samples=1000,\n",
    "                                                        number_features=NUMBER_OF_FEATURES,\n",
    "                                                        noise_value=0.5)\n",
    "print(f'shape of train: {X_train.shape, y_train.shape}\\nshape of test: {X_test.shape, y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (torch.Size([800, 10]), torch.Size([800, 1]))\n",
      "shape of test: (torch.Size([200, 10]), torch.Size([200, 1]))\n",
      "example of train sample:\n",
      " (tensor([ 0.5686,  0.6552,  1.6264,  0.3829, -1.3236,  0.2570,  0.2360, -0.4359,\n",
      "         0.3517,  0.8775]), tensor([176.7505]))\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of train: {X_train.shape, y_train.shape}\\nshape of test: {X_test.shape, y_test.shape}')\n",
    "train_dataset = RegressionDataset(features=X_train,\n",
    "                                  labels=y_train)\n",
    "test_dataset = RegressionDataset(features=X_test,\n",
    "                                 labels=y_test)\n",
    "print(f'example of train sample:\\n {train_dataset[21]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch: features - torch.Size([10, 10]) and labels - torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "batch_example_features, batch_example_labels  = next(iter(train_dataloader))\n",
    "print('shape of batch: features - {} and labels - {}'.format(batch_example_features.shape, batch_example_labels.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and score function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "metric_fn = R2Score()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.fc_first = nn.Linear(10, 65)\n",
    "        self.fc_main = nn.Sequential(nn.ReLU(),\n",
    "                                     nn.Linear(65, 10),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(10, 1))\n",
    "        # self.fc_last = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_first(x)\n",
    "        x = self.fc_main(x)\n",
    "        # x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "model = ComplexNet().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "optimizer_adam = Adam(params=model.fc_main.parameters(), lr=0.01)\n",
    "\n",
    "optimizer_drem = DREMOptimizer(params=model.fc_first.parameters(),\n",
    "                               lr=1e-20)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train: DREM and Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the 1th epoch: loss = 15132677120.000 & score = -0.0988222062587738\n",
      "on the 2th epoch: loss = 4652124672.000 & score = -0.09212847799062729\n",
      "on the 3th epoch: loss = 8443752448.000 & score = -0.06572342664003372\n",
      "on the 4th epoch: loss = 14265401344.000 & score = -0.03978119418025017\n",
      "on the 5th epoch: loss = 7221962240.000 & score = -0.001651135040447116\n",
      "on the 6th epoch: loss = 4490747904.000 & score = 0.02217349037528038\n",
      "on the 7th epoch: loss = 8875200512.000 & score = 0.04214435815811157\n",
      "on the 8th epoch: loss = 10656182272.000 & score = 0.04947804659605026\n",
      "on the 9th epoch: loss = 3448438784.000 & score = 0.0646325945854187\n",
      "on the 10th epoch: loss = 7600753664.000 & score = 0.08721859753131866\n",
      "on the 11th epoch: loss = 5163182592.000 & score = 0.10807178169488907\n",
      "on the 12th epoch: loss = 4632870912.000 & score = 0.1255250722169876\n",
      "on the 13th epoch: loss = 6246724608.000 & score = 0.1600666642189026\n",
      "on the 14th epoch: loss = 9062263808.000 & score = 0.21324582397937775\n",
      "on the 15th epoch: loss = 6453016576.000 & score = 0.25619229674339294\n",
      "on the 16th epoch: loss = 10044767232.000 & score = 0.29365962743759155\n",
      "on the 17th epoch: loss = 37855780864.000 & score = 0.30814725160598755\n",
      "on the 18th epoch: loss = 3097716224.000 & score = 0.3252483308315277\n",
      "on the 19th epoch: loss = 5109308928.000 & score = 0.32977503538131714\n",
      "on the 20th epoch: loss = 5242836992.000 & score = 0.3395138382911682\n",
      "on the 21th epoch: loss = 8157293568.000 & score = 0.35408979654312134\n",
      "on the 22th epoch: loss = 3725258752.000 & score = 0.36385616660118103\n",
      "on the 23th epoch: loss = 2272409344.000 & score = 0.36936309933662415\n",
      "on the 24th epoch: loss = 2429976832.000 & score = 0.37549328804016113\n",
      "on the 25th epoch: loss = 5126864896.000 & score = 0.38866865634918213\n",
      "on the 26th epoch: loss = 7806287872.000 & score = 0.4066672921180725\n",
      "on the 27th epoch: loss = 2044861696.000 & score = 0.4150853157043457\n",
      "on the 28th epoch: loss = 3354396928.000 & score = 0.42049703001976013\n",
      "on the 29th epoch: loss = 3630957824.000 & score = 0.4371045231819153\n",
      "on the 30th epoch: loss = 2275584256.000 & score = 0.44365254044532776\n",
      "on the 31th epoch: loss = 3033583104.000 & score = 0.45446139574050903\n",
      "on the 32th epoch: loss = 3309774080.000 & score = 0.4567042887210846\n",
      "on the 33th epoch: loss = 3339375104.000 & score = 0.46761375665664673\n",
      "on the 34th epoch: loss = 3122495744.000 & score = 0.4721841812133789\n",
      "on the 35th epoch: loss = 2652064256.000 & score = 0.4772581160068512\n",
      "on the 36th epoch: loss = 2254992384.000 & score = 0.4832121729850769\n",
      "on the 37th epoch: loss = 7899534336.000 & score = 0.5198147296905518\n",
      "on the 38th epoch: loss = 3895273984.000 & score = 0.5383471846580505\n",
      "on the 39th epoch: loss = 2247706112.000 & score = 0.5473914742469788\n",
      "on the 40th epoch: loss = 3329362176.000 & score = 0.5499256253242493\n",
      "on the 41th epoch: loss = 4212157184.000 & score = 0.5656026601791382\n",
      "on the 42th epoch: loss = 2304732672.000 & score = 0.5735620260238647\n",
      "on the 43th epoch: loss = 2555410944.000 & score = 0.581345796585083\n",
      "on the 44th epoch: loss = 6399085568.000 & score = 0.5991683602333069\n",
      "on the 45th epoch: loss = 2844883456.000 & score = 0.6079773902893066\n",
      "on the 46th epoch: loss = 4260533760.000 & score = 0.6063516736030579\n",
      "on the 47th epoch: loss = 5146891264.000 & score = 0.61690354347229\n",
      "on the 48th epoch: loss = 9297234944.000 & score = 0.6256024837493896\n",
      "on the 49th epoch: loss = 4328642560.000 & score = 0.6304353475570679\n",
      "on the 50th epoch: loss = 2513858304.000 & score = 0.6410861015319824\n",
      "on the 51th epoch: loss = 3274583808.000 & score = 0.6477085947990417\n",
      "on the 52th epoch: loss = 2825373184.000 & score = 0.6274715662002563\n",
      "on the 53th epoch: loss = 4685565952.000 & score = 0.6646460294723511\n",
      "on the 54th epoch: loss = 3124974080.000 & score = 0.6659886837005615\n",
      "on the 55th epoch: loss = 6144535552.000 & score = 0.6847655177116394\n",
      "on the 56th epoch: loss = 3838247680.000 & score = 0.7035092711448669\n",
      "on the 57th epoch: loss = 2381876480.000 & score = 0.6893100738525391\n",
      "on the 58th epoch: loss = 2110354688.000 & score = 0.7142132520675659\n",
      "on the 59th epoch: loss = 4102500864.000 & score = 0.7043331861495972\n",
      "on the 60th epoch: loss = 10486872064.000 & score = 0.7461596131324768\n",
      "on the 61th epoch: loss = 3786940928.000 & score = 0.7564748525619507\n",
      "on the 62th epoch: loss = 6597074944.000 & score = 0.748388409614563\n",
      "on the 63th epoch: loss = 1588350720.000 & score = 0.7509864568710327\n",
      "on the 64th epoch: loss = 3165555712.000 & score = 0.7594557404518127\n",
      "on the 65th epoch: loss = 2118804096.000 & score = 0.7566791772842407\n",
      "on the 66th epoch: loss = 1606175616.000 & score = 0.7556154131889343\n",
      "on the 67th epoch: loss = 1799593344.000 & score = 0.7635828256607056\n",
      "on the 68th epoch: loss = 1640914688.000 & score = 0.7701858282089233\n",
      "on the 69th epoch: loss = 2115512704.000 & score = 0.7691045999526978\n",
      "on the 70th epoch: loss = 2055356160.000 & score = 0.7758404612541199\n",
      "on the 71th epoch: loss = 1549688064.000 & score = 0.7803688645362854\n",
      "on the 72th epoch: loss = 3218504192.000 & score = 0.7802583575248718\n",
      "on the 73th epoch: loss = 1615052800.000 & score = 0.7831639647483826\n",
      "on the 74th epoch: loss = 1694192896.000 & score = 0.7726835608482361\n",
      "on the 75th epoch: loss = 1346420352.000 & score = 0.774021327495575\n",
      "on the 76th epoch: loss = 3398247936.000 & score = 0.7709611654281616\n",
      "on the 77th epoch: loss = 1743514880.000 & score = 0.7722389101982117\n",
      "on the 78th epoch: loss = 2145929856.000 & score = 0.780794620513916\n",
      "on the 79th epoch: loss = 1893705728.000 & score = 0.7395912408828735\n",
      "on the 80th epoch: loss = 7013276672.000 & score = 0.7755576372146606\n",
      "on the 81th epoch: loss = 2038580864.000 & score = 0.7818547487258911\n",
      "on the 82th epoch: loss = 2489547008.000 & score = 0.7742484211921692\n",
      "on the 83th epoch: loss = 1564459904.000 & score = 0.777774453163147\n",
      "on the 84th epoch: loss = 2232451328.000 & score = 0.7838681936264038\n",
      "on the 85th epoch: loss = 1081913984.000 & score = 0.7894052863121033\n",
      "on the 86th epoch: loss = 2915244544.000 & score = 0.7873932719230652\n",
      "on the 87th epoch: loss = 2431581696.000 & score = 0.7752428650856018\n",
      "on the 88th epoch: loss = 4038206976.000 & score = 0.7935123443603516\n",
      "on the 89th epoch: loss = 1639837696.000 & score = 0.7998225688934326\n",
      "on the 90th epoch: loss = 1111463168.000 & score = 0.8046636581420898\n",
      "on the 91th epoch: loss = 2821684480.000 & score = 0.7995997667312622\n",
      "on the 92th epoch: loss = 1619391744.000 & score = 0.8095739483833313\n",
      "on the 93th epoch: loss = 1179168768.000 & score = 0.8048578500747681\n",
      "on the 94th epoch: loss = 2367826432.000 & score = 0.8013188242912292\n",
      "on the 95th epoch: loss = 3204201472.000 & score = 0.8061405420303345\n",
      "on the 96th epoch: loss = 1532880640.000 & score = 0.7967301607131958\n",
      "on the 97th epoch: loss = 2039853440.000 & score = 0.807176411151886\n",
      "on the 98th epoch: loss = 1114174976.000 & score = 0.8095763325691223\n",
      "on the 99th epoch: loss = 2168668672.000 & score = 0.793813943862915\n",
      "on the 100th epoch: loss = 1584566784.000 & score = 0.7882779836654663\n",
      "on the 101th epoch: loss = 1270649088.000 & score = 0.7990044355392456\n",
      "on the 102th epoch: loss = 4820702208.000 & score = 0.8391119241714478\n",
      "on the 103th epoch: loss = 1070312256.000 & score = 0.8417647480964661\n",
      "on the 104th epoch: loss = 2441395712.000 & score = 0.8322680592536926\n",
      "on the 105th epoch: loss = 3360638720.000 & score = 0.7922499179840088\n",
      "on the 106th epoch: loss = 3105960704.000 & score = 0.8380848169326782\n",
      "on the 107th epoch: loss = 2537977344.000 & score = 0.83966463804245\n",
      "on the 108th epoch: loss = 2587879680.000 & score = 0.8373199701309204\n",
      "on the 109th epoch: loss = 7452070912.000 & score = 0.80182284116745\n",
      "on the 110th epoch: loss = 2087008896.000 & score = 0.8139853477478027\n",
      "on the 111th epoch: loss = 3954470144.000 & score = 0.8198049664497375\n",
      "on the 112th epoch: loss = 1497324288.000 & score = 0.8212070465087891\n",
      "on the 113th epoch: loss = 1631170048.000 & score = 0.819916844367981\n",
      "on the 114th epoch: loss = 1707268736.000 & score = 0.8222578167915344\n",
      "on the 115th epoch: loss = 2586179072.000 & score = 0.8264859914779663\n",
      "on the 116th epoch: loss = 1688146560.000 & score = 0.8275837898254395\n",
      "on the 117th epoch: loss = 1550887168.000 & score = 0.8224694132804871\n",
      "on the 118th epoch: loss = 2598658560.000 & score = 0.8227180242538452\n",
      "on the 119th epoch: loss = 1872114944.000 & score = 0.8205512166023254\n",
      "on the 120th epoch: loss = 3106829056.000 & score = 0.8232383728027344\n",
      "on the 121th epoch: loss = 2251498496.000 & score = 0.8168319463729858\n",
      "on the 122th epoch: loss = 1916745472.000 & score = 0.8168686628341675\n",
      "on the 123th epoch: loss = 898519936.000 & score = 0.8235966563224792\n",
      "on the 124th epoch: loss = 1349146112.000 & score = 0.8285804986953735\n",
      "on the 125th epoch: loss = 1749427968.000 & score = 0.8364019393920898\n",
      "on the 126th epoch: loss = 4465778688.000 & score = 0.8328443765640259\n",
      "on the 127th epoch: loss = 1577543552.000 & score = 0.8322631120681763\n",
      "on the 128th epoch: loss = 2202404864.000 & score = 0.8344809412956238\n",
      "on the 129th epoch: loss = 1087695232.000 & score = 0.8380417823791504\n",
      "on the 130th epoch: loss = 1388447360.000 & score = 0.8405609130859375\n",
      "on the 131th epoch: loss = 1161683200.000 & score = 0.8455469012260437\n",
      "on the 132th epoch: loss = 1973784576.000 & score = 0.8360593914985657\n",
      "on the 133th epoch: loss = 1358368512.000 & score = 0.8372683525085449\n",
      "on the 134th epoch: loss = 781865664.000 & score = 0.8369304537773132\n",
      "on the 135th epoch: loss = 2355750144.000 & score = 0.840844452381134\n",
      "on the 136th epoch: loss = 1176124800.000 & score = 0.8274283409118652\n",
      "on the 137th epoch: loss = 1434648320.000 & score = 0.8380781412124634\n",
      "on the 138th epoch: loss = 1650412800.000 & score = 0.8479955792427063\n",
      "on the 139th epoch: loss = 864288128.000 & score = 0.844840407371521\n",
      "on the 140th epoch: loss = 1442066304.000 & score = 0.8419439196586609\n",
      "on the 141th epoch: loss = 1837028352.000 & score = 0.8483001589775085\n",
      "on the 142th epoch: loss = 1274918656.000 & score = 0.8551217913627625\n",
      "on the 143th epoch: loss = 1202992512.000 & score = 0.8436762094497681\n",
      "on the 144th epoch: loss = 1208487936.000 & score = 0.8522637486457825\n",
      "on the 145th epoch: loss = 1085250176.000 & score = 0.8515046238899231\n",
      "on the 146th epoch: loss = 2025933184.000 & score = 0.851162314414978\n",
      "on the 147th epoch: loss = 1248658688.000 & score = 0.8509446382522583\n",
      "on the 148th epoch: loss = 970866880.000 & score = 0.8610677719116211\n",
      "on the 149th epoch: loss = 1574102272.000 & score = 0.8449780344963074\n",
      "on the 150th epoch: loss = 2158720512.000 & score = 0.8628911972045898\n",
      "on the 151th epoch: loss = 2177602816.000 & score = 0.8686412572860718\n",
      "on the 152th epoch: loss = 1229349504.000 & score = 0.8608805537223816\n",
      "on the 153th epoch: loss = 884624192.000 & score = 0.8662552833557129\n",
      "on the 154th epoch: loss = 968128128.000 & score = 0.8658279180526733\n",
      "on the 155th epoch: loss = 1474569984.000 & score = 0.8626232147216797\n",
      "on the 156th epoch: loss = 1740703744.000 & score = 0.8591939210891724\n",
      "on the 157th epoch: loss = 4615359488.000 & score = 0.8293125033378601\n",
      "on the 158th epoch: loss = 2011635968.000 & score = 0.8558389544487\n",
      "on the 159th epoch: loss = 1028836032.000 & score = 0.8515759706497192\n",
      "on the 160th epoch: loss = 2390579712.000 & score = 0.8535059094429016\n",
      "on the 161th epoch: loss = 1293297408.000 & score = 0.8469526171684265\n",
      "on the 162th epoch: loss = 2473932800.000 & score = 0.8385359048843384\n",
      "on the 163th epoch: loss = 2100948736.000 & score = 0.8449357151985168\n",
      "on the 164th epoch: loss = 1777785472.000 & score = 0.8495303988456726\n",
      "on the 165th epoch: loss = 859684544.000 & score = 0.8507353067398071\n",
      "on the 166th epoch: loss = 1566029824.000 & score = 0.851887583732605\n",
      "on the 167th epoch: loss = 1006681216.000 & score = 0.8464347124099731\n",
      "on the 168th epoch: loss = 1727374336.000 & score = 0.8520305752754211\n",
      "on the 169th epoch: loss = 1136648448.000 & score = 0.8535143136978149\n",
      "on the 170th epoch: loss = 2404387328.000 & score = 0.8714700937271118\n",
      "on the 171th epoch: loss = 1439368320.000 & score = 0.8814266920089722\n",
      "on the 172th epoch: loss = 2763388416.000 & score = 0.8543232679367065\n",
      "on the 173th epoch: loss = 1198182656.000 & score = 0.8523570895195007\n",
      "on the 174th epoch: loss = 1572991488.000 & score = 0.8501399159431458\n",
      "on the 175th epoch: loss = 2181644800.000 & score = 0.8551117181777954\n",
      "on the 176th epoch: loss = 2263837184.000 & score = 0.8288923501968384\n",
      "on the 177th epoch: loss = 2518028032.000 & score = 0.8585478663444519\n",
      "on the 178th epoch: loss = 1754664960.000 & score = 0.8726421594619751\n",
      "on the 179th epoch: loss = 1386907392.000 & score = 0.8766983151435852\n",
      "on the 180th epoch: loss = 1327897472.000 & score = 0.8720613718032837\n",
      "on the 181th epoch: loss = 1348021760.000 & score = 0.8731284141540527\n",
      "on the 182th epoch: loss = 676320896.000 & score = 0.8756012916564941\n",
      "on the 183th epoch: loss = 2317624832.000 & score = 0.8890681266784668\n",
      "on the 184th epoch: loss = 2406460928.000 & score = 0.8895043134689331\n",
      "on the 185th epoch: loss = 2799688960.000 & score = 0.879796028137207\n",
      "on the 186th epoch: loss = 1027855040.000 & score = 0.8844658136367798\n",
      "on the 187th epoch: loss = 1565397376.000 & score = 0.8786945343017578\n",
      "on the 188th epoch: loss = 1305291136.000 & score = 0.88379967212677\n",
      "on the 189th epoch: loss = 892583616.000 & score = 0.8844215273857117\n",
      "on the 190th epoch: loss = 2446435840.000 & score = 0.8554609417915344\n",
      "on the 191th epoch: loss = 899553600.000 & score = 0.8572502136230469\n",
      "on the 192th epoch: loss = 2094210688.000 & score = 0.8618383407592773\n",
      "on the 193th epoch: loss = 1047070848.000 & score = 0.8664999008178711\n",
      "on the 194th epoch: loss = 1009452416.000 & score = 0.8667117357254028\n",
      "on the 195th epoch: loss = 883194368.000 & score = 0.8589125871658325\n",
      "on the 196th epoch: loss = 1987852032.000 & score = 0.8697040677070618\n",
      "on the 197th epoch: loss = 1293031808.000 & score = 0.8625176548957825\n",
      "on the 198th epoch: loss = 3110669312.000 & score = 0.8662987947463989\n",
      "on the 199th epoch: loss = 2764947456.000 & score = 0.864166259765625\n",
      "on the 200th epoch: loss = 3044692992.000 & score = 0.8845402002334595\n",
      "on the 201th epoch: loss = 1393188608.000 & score = 0.8859131932258606\n",
      "on the 202th epoch: loss = 1232413440.000 & score = 0.8765958547592163\n",
      "on the 203th epoch: loss = 1076699904.000 & score = 0.8808332681655884\n",
      "on the 204th epoch: loss = 1680012288.000 & score = 0.8810547590255737\n",
      "on the 205th epoch: loss = 1280660864.000 & score = 0.8778418302536011\n",
      "on the 206th epoch: loss = 1443958144.000 & score = 0.8778525590896606\n",
      "on the 207th epoch: loss = 1668513024.000 & score = 0.8818188905715942\n",
      "on the 208th epoch: loss = 5078209024.000 & score = 0.9107955098152161\n",
      "on the 209th epoch: loss = 3149994240.000 & score = 0.897642970085144\n",
      "on the 210th epoch: loss = 1313646464.000 & score = 0.8837536573410034\n",
      "on the 211th epoch: loss = 2335065856.000 & score = 0.8816226124763489\n",
      "on the 212th epoch: loss = 1573521152.000 & score = 0.8895999193191528\n",
      "on the 213th epoch: loss = 1578739456.000 & score = 0.865574836730957\n",
      "on the 214th epoch: loss = 2303160832.000 & score = 0.8886396288871765\n",
      "on the 215th epoch: loss = 1973694080.000 & score = 0.8794323205947876\n",
      "on the 216th epoch: loss = 869670912.000 & score = 0.8772058486938477\n",
      "on the 217th epoch: loss = 989547648.000 & score = 0.8876490592956543\n",
      "on the 218th epoch: loss = 1513433344.000 & score = 0.8901301622390747\n",
      "on the 219th epoch: loss = 6251408896.000 & score = 0.8437311053276062\n",
      "on the 220th epoch: loss = 1195663104.000 & score = 0.8662756085395813\n",
      "on the 221th epoch: loss = 3500942080.000 & score = 0.8563291430473328\n",
      "on the 222th epoch: loss = 1471741056.000 & score = 0.8613837957382202\n",
      "on the 223th epoch: loss = 1461280000.000 & score = 0.8677508234977722\n",
      "on the 224th epoch: loss = 1150372224.000 & score = 0.8685426712036133\n",
      "on the 225th epoch: loss = 898807168.000 & score = 0.8671258687973022\n",
      "on the 226th epoch: loss = 1474880256.000 & score = 0.8666707873344421\n",
      "on the 227th epoch: loss = 1572110080.000 & score = 0.8604440689086914\n",
      "on the 228th epoch: loss = 1619309440.000 & score = 0.8588767051696777\n",
      "on the 229th epoch: loss = 1156569088.000 & score = 0.8669916391372681\n",
      "on the 230th epoch: loss = 5798613504.000 & score = 0.8489592671394348\n",
      "on the 231th epoch: loss = 2345299712.000 & score = 0.8617399334907532\n",
      "on the 232th epoch: loss = 2057247744.000 & score = 0.8663653135299683\n",
      "on the 233th epoch: loss = 1117955328.000 & score = 0.8724458813667297\n",
      "on the 234th epoch: loss = 901549184.000 & score = 0.8793627023696899\n",
      "on the 235th epoch: loss = 5597717504.000 & score = 0.8734977841377258\n",
      "on the 236th epoch: loss = 1297862400.000 & score = 0.8639494776725769\n",
      "on the 237th epoch: loss = 1044594176.000 & score = 0.8716756701469421\n",
      "on the 238th epoch: loss = 1267764224.000 & score = 0.8726747632026672\n",
      "on the 239th epoch: loss = 2551992320.000 & score = 0.8608205914497375\n",
      "on the 240th epoch: loss = 1163639552.000 & score = 0.8779786825180054\n",
      "on the 241th epoch: loss = 2898254336.000 & score = 0.8683115839958191\n",
      "on the 242th epoch: loss = 1108199936.000 & score = 0.8740922212600708\n",
      "on the 243th epoch: loss = 1488347392.000 & score = 0.8679930567741394\n",
      "on the 244th epoch: loss = 939841024.000 & score = 0.8667358160018921\n",
      "on the 245th epoch: loss = 710271488.000 & score = 0.8687275648117065\n",
      "on the 246th epoch: loss = 667864704.000 & score = 0.8686224818229675\n",
      "on the 247th epoch: loss = 2392252928.000 & score = 0.8638924360275269\n",
      "on the 248th epoch: loss = 2388570368.000 & score = 0.8651153445243835\n",
      "on the 249th epoch: loss = 829251072.000 & score = 0.8600395917892456\n",
      "on the 250th epoch: loss = 2912084480.000 & score = 0.8712598085403442\n",
      "on the 251th epoch: loss = 1461086720.000 & score = 0.8627501726150513\n",
      "on the 252th epoch: loss = 1774024960.000 & score = 0.8682481050491333\n",
      "on the 253th epoch: loss = 2602019072.000 & score = 0.8568115234375\n",
      "on the 254th epoch: loss = 1458708864.000 & score = 0.8803257942199707\n",
      "on the 255th epoch: loss = 1728494976.000 & score = 0.8838878870010376\n",
      "on the 256th epoch: loss = 1831017472.000 & score = 0.8947874903678894\n",
      "on the 257th epoch: loss = 2599721472.000 & score = 0.851017951965332\n",
      "on the 258th epoch: loss = 2369884928.000 & score = 0.88972407579422\n",
      "on the 259th epoch: loss = 3626486272.000 & score = 0.8911482095718384\n",
      "on the 260th epoch: loss = 2191297536.000 & score = 0.8791972398757935\n",
      "on the 261th epoch: loss = 1321782400.000 & score = 0.8921071290969849\n",
      "on the 262th epoch: loss = 909388288.000 & score = 0.8939045071601868\n",
      "on the 263th epoch: loss = 1119119104.000 & score = 0.893933892250061\n",
      "on the 264th epoch: loss = 1604061952.000 & score = 0.8915338516235352\n",
      "on the 265th epoch: loss = 2466264064.000 & score = 0.8686664700508118\n",
      "on the 266th epoch: loss = 4585056256.000 & score = 0.8381015658378601\n",
      "on the 267th epoch: loss = 3353254400.000 & score = 0.8659125566482544\n",
      "on the 268th epoch: loss = 1621947904.000 & score = 0.8620947599411011\n",
      "on the 269th epoch: loss = 2170446592.000 & score = 0.8645071983337402\n",
      "on the 270th epoch: loss = 1271607040.000 & score = 0.8673915863037109\n",
      "on the 271th epoch: loss = 1294221952.000 & score = 0.8669450879096985\n",
      "on the 272th epoch: loss = 2960591872.000 & score = 0.8718472719192505\n",
      "on the 273th epoch: loss = 1070121472.000 & score = 0.8682193756103516\n",
      "on the 274th epoch: loss = 2540317696.000 & score = 0.8593446016311646\n",
      "on the 275th epoch: loss = 729363392.000 & score = 0.8719728589057922\n",
      "on the 276th epoch: loss = 1117856128.000 & score = 0.8747679591178894\n",
      "on the 277th epoch: loss = 830888640.000 & score = 0.8778258562088013\n",
      "on the 278th epoch: loss = 704573632.000 & score = 0.8766970634460449\n",
      "on the 279th epoch: loss = 1145341184.000 & score = 0.8723184466362\n",
      "on the 280th epoch: loss = 2512208384.000 & score = 0.8766739964485168\n",
      "on the 281th epoch: loss = 2845074432.000 & score = 0.8779122233390808\n",
      "on the 282th epoch: loss = 867713920.000 & score = 0.881266713142395\n",
      "on the 283th epoch: loss = 961738560.000 & score = 0.8780150413513184\n",
      "on the 284th epoch: loss = 1518657408.000 & score = 0.8779748678207397\n",
      "on the 285th epoch: loss = 3164172800.000 & score = 0.778765082359314\n",
      "on the 286th epoch: loss = 6949167104.000 & score = 0.8782554864883423\n",
      "on the 287th epoch: loss = 1362294528.000 & score = 0.8832939267158508\n",
      "on the 288th epoch: loss = 3475578368.000 & score = 0.8768070340156555\n",
      "on the 289th epoch: loss = 1977641984.000 & score = 0.8812999725341797\n",
      "on the 290th epoch: loss = 1264547456.000 & score = 0.8830037117004395\n",
      "on the 291th epoch: loss = 977087616.000 & score = 0.8832823634147644\n",
      "on the 292th epoch: loss = 1174902784.000 & score = 0.8785891532897949\n",
      "on the 293th epoch: loss = 1227072768.000 & score = 0.836910605430603\n",
      "on the 294th epoch: loss = 1138301184.000 & score = 0.874017596244812\n",
      "on the 295th epoch: loss = 1068057088.000 & score = 0.8783788681030273\n",
      "on the 296th epoch: loss = 1026878976.000 & score = 0.8793059587478638\n",
      "on the 297th epoch: loss = 1434837376.000 & score = 0.8778400421142578\n",
      "on the 298th epoch: loss = 1485623936.000 & score = 0.8865655660629272\n",
      "on the 299th epoch: loss = 823793472.000 & score = 0.8878809213638306\n",
      "on the 300th epoch: loss = 1803640832.000 & score = 0.881732165813446\n",
      "on the 301th epoch: loss = 3285900032.000 & score = 0.8671383857727051\n",
      "on the 302th epoch: loss = 3842692352.000 & score = 0.8713995814323425\n",
      "on the 303th epoch: loss = 1015922880.000 & score = 0.8750241994857788\n",
      "on the 304th epoch: loss = 1109958016.000 & score = 0.8773935437202454\n",
      "on the 305th epoch: loss = 1148271232.000 & score = 0.8695535659790039\n",
      "on the 306th epoch: loss = 1605675264.000 & score = 0.8866289854049683\n",
      "on the 307th epoch: loss = 1852972416.000 & score = 0.868109405040741\n",
      "on the 308th epoch: loss = 1697847040.000 & score = 0.8777451515197754\n",
      "on the 309th epoch: loss = 4073346304.000 & score = 0.8887124061584473\n",
      "on the 310th epoch: loss = 1114491648.000 & score = 0.8839956521987915\n",
      "on the 311th epoch: loss = 2381683200.000 & score = 0.8481916189193726\n",
      "on the 312th epoch: loss = 2032692224.000 & score = 0.8826258778572083\n",
      "on the 313th epoch: loss = 904556544.000 & score = 0.8814945220947266\n",
      "on the 314th epoch: loss = 913716736.000 & score = 0.8821322321891785\n",
      "on the 315th epoch: loss = 929523200.000 & score = 0.883409321308136\n",
      "on the 316th epoch: loss = 1339635072.000 & score = 0.8833950161933899\n",
      "on the 317th epoch: loss = 4061495296.000 & score = 0.8359915018081665\n",
      "on the 318th epoch: loss = 1387652608.000 & score = 0.8844629526138306\n",
      "on the 319th epoch: loss = 2172292864.000 & score = 0.8868575096130371\n",
      "on the 320th epoch: loss = 1594088320.000 & score = 0.8912442922592163\n",
      "on the 321th epoch: loss = 1511628032.000 & score = 0.8895702362060547\n",
      "on the 322th epoch: loss = 906856448.000 & score = 0.8873764872550964\n",
      "on the 323th epoch: loss = 1767448576.000 & score = 0.8877694010734558\n",
      "on the 324th epoch: loss = 1495013632.000 & score = 0.844089686870575\n",
      "on the 325th epoch: loss = 1289428224.000 & score = 0.8930627107620239\n",
      "on the 326th epoch: loss = 1913011840.000 & score = 0.8874279260635376\n",
      "on the 327th epoch: loss = 975761536.000 & score = 0.8918685913085938\n",
      "on the 328th epoch: loss = 1379279616.000 & score = 0.8851150274276733\n",
      "on the 329th epoch: loss = 2161913600.000 & score = 0.8690472841262817\n",
      "on the 330th epoch: loss = 1303852800.000 & score = 0.8887976408004761\n",
      "on the 331th epoch: loss = 1166660096.000 & score = 0.8906394839286804\n",
      "on the 332th epoch: loss = 907287040.000 & score = 0.893840491771698\n",
      "on the 333th epoch: loss = 6258824192.000 & score = 0.8819897770881653\n",
      "on the 334th epoch: loss = 1478749184.000 & score = 0.8936360478401184\n",
      "on the 335th epoch: loss = 1122633088.000 & score = 0.893642246723175\n",
      "on the 336th epoch: loss = 833330688.000 & score = 0.8971375226974487\n",
      "on the 337th epoch: loss = 5412115456.000 & score = 0.7959257364273071\n",
      "on the 338th epoch: loss = 4279799040.000 & score = 0.8826858401298523\n",
      "on the 339th epoch: loss = 1254579968.000 & score = 0.8824652433395386\n",
      "on the 340th epoch: loss = 1086715136.000 & score = 0.8829188346862793\n",
      "on the 341th epoch: loss = 701756800.000 & score = 0.8836954236030579\n",
      "on the 342th epoch: loss = 1355877632.000 & score = 0.883264422416687\n",
      "on the 343th epoch: loss = 1485196032.000 & score = 0.8833758234977722\n",
      "on the 344th epoch: loss = 2550094848.000 & score = 0.8874508738517761\n",
      "on the 345th epoch: loss = 1461753088.000 & score = 0.8889321088790894\n",
      "on the 346th epoch: loss = 1150107776.000 & score = 0.8923854827880859\n",
      "on the 347th epoch: loss = 1946090240.000 & score = 0.8906968832015991\n",
      "on the 348th epoch: loss = 1795952256.000 & score = 0.8908979296684265\n",
      "on the 349th epoch: loss = 1246391040.000 & score = 0.8909056782722473\n",
      "on the 350th epoch: loss = 929631616.000 & score = 0.898138165473938\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 350\n",
    "new_loss = True\n",
    "score_val, loss_val = [], []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # train\n",
    "    loss_epoch = 0.0\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        predicted = model(X_batch)\n",
    "        if new_loss:\n",
    "            determinant = torch.det(X_batch)\n",
    "            inverse_batch = torch.linalg.inv(X_batch)\n",
    "            adjoin = determinant * inverse_batch\n",
    "            loss = loss_fn(adjoin @ predicted, adjoin @ y_batch)\n",
    "        else:\n",
    "            loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        loss_epoch += loss.detach()\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer_drem.zero_grad()\n",
    "        optimizer_adam.zero_grad()\n",
    "        # optimizer3.zero_grad()\n",
    "\n",
    "        # backpropagation (compute gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        # optimizer3.step()\n",
    "        optimizer_drem.step(det_batch=determinant)\n",
    "        optimizer_adam.step()\n",
    "\n",
    "    # evaluate\n",
    "    mean_loss_test, mean_metric_test = validation_epoch(model, loss_fn, metric_fn, test_dataloader, \"cpu\")\n",
    "    loss_val.append(mean_loss_test)\n",
    "    score_val.append(mean_metric_test)\n",
    "    print(f'on the {epoch}th epoch: loss = {(loss_epoch) / (len(train_dataloader)):.3f} & score = {mean_metric_test}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FInite-Time optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.fc_first = nn.Linear(10, 45)\n",
    "        self.fc_main = nn.Sequential(nn.ReLU(),\n",
    "                                     nn.Linear(45, 25),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(25, 1),\n",
    "                                     nn.ReLU()\n",
    "                                     )\n",
    "        # self.fc_last = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_first(x)\n",
    "        x = self.fc_main(x)\n",
    "        # x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "model = ComplexNet().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "N_OF_BATCHES = 25\n",
    "optimizer_ft = FiniteTimeOptimizer(params=model.fc_first.parameters(),\n",
    "                                    lr=1e-15,\n",
    "                                    n_of_batches=N_OF_BATCHES)\n",
    "\n",
    "# optimizer2 = SGD(params=model.fc_last.parameters(),\n",
    "#                  lr=0.01)\n",
    "\n",
    "optimizer_adam = Adam(params=model.fc_main.parameters(),\n",
    "                     lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the 1th epoch: loss = 1721425641015869440.000 & score = -0.11156518757343292\n",
      "on the 2th epoch: loss = 7638746624.000 & score = -0.11156518757343292\n",
      "can not apply\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 350\n",
    "new_loss = True\n",
    "score_val, loss_val = [], []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # train\n",
    "    loss_epoch = 0.0\n",
    "    model.train()\n",
    "    for batch_num, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        predicted = model(X_batch)\n",
    "        if new_loss:\n",
    "            determinant = torch.det(X_batch)\n",
    "            inverse_batch = torch.linalg.inv(X_batch)\n",
    "            adjoin = determinant * inverse_batch\n",
    "            loss = loss_fn(adjoin @ predicted, adjoin @ y_batch)\n",
    "        else:\n",
    "            loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        loss_epoch += loss.detach()\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer_ft.zero_grad()\n",
    "        optimizer_adam.zero_grad()\n",
    "        # optimizer3.zero_grad()\n",
    "\n",
    "        # backpropagation (compute gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        # optimizer3.step()\n",
    "        optimizer_ft.step(det_batch=determinant, t=batch_num+1)\n",
    "        optimizer_adam.step()\n",
    "\n",
    "    # evaluate\n",
    "    mean_loss_test, mean_metric_test = validation_epoch(model, loss_fn, metric_fn, test_dataloader, \"cpu\")\n",
    "    loss_val.append(mean_loss_test)\n",
    "    score_val.append(mean_metric_test)\n",
    "\n",
    "    if torch.isnan(mean_metric_test):\n",
    "        print('can not apply')\n",
    "        break\n",
    "\n",
    "    print(f'on the {epoch}th epoch: loss = {(loss_epoch) / (len(train_dataloader)):.3f} & score = {mean_metric_test}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "#### gjxtve kb,thvfkmyst dpukzs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        # self.fc_first = nn.Linear(10, 45)\n",
    "        self.fc_main = nn.Sequential(nn.Linear(10, 45),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(45, 25),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(25, 10),\n",
    "                                     nn.ReLU())\n",
    "        self.fc_last = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc_first(x)\n",
    "        x = self.fc_main(x)\n",
    "        x = self.fc_last(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "model = ComplexNet().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "N_OF_BATCHES = 35\n",
    "optimizer_ft = FiniteTimeOptimizer(params=model.fc_last.parameters(),\n",
    "                                    lr=1e-11,\n",
    "                                    n_of_batches=N_OF_BATCHES)\n",
    "\n",
    "# optimizer2 = SGD(params=model.fc_last.parameters(),\n",
    "#                  lr=0.01)\n",
    "\n",
    "optimizer_adam = Adam(params=model.fc_main.parameters(),\n",
    "                     lr=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the 1th epoch: loss = 7577798144.000 & score = -0.09028781950473785\n",
      "on the 2th epoch: loss = 268553567600640000.000 & score = -18275090.0\n",
      "on the 3th epoch: loss = 48960093741842879649677312.000 & score = -1.192974947975168e+16\n",
      "on the 4th epoch: loss = 4863629348834735949756015893807104.000 & score = -1.269344671063551e+24\n",
      "on the 5th epoch: loss = inf & score = -3.1029039620237233e+31\n",
      "on the 6th epoch: loss = inf & score = -inf\n",
      "on the 7th epoch: loss = inf & score = -inf\n",
      "on the 8th epoch: loss = inf & score = -inf\n",
      "on the 9th epoch: loss = inf & score = -inf\n",
      "can not apply\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 350\n",
    "new_loss = True\n",
    "score_val, loss_val = [], []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # train\n",
    "    loss_epoch = 0.0\n",
    "    model.train()\n",
    "    for batch_num, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        predicted = model(X_batch)\n",
    "        if new_loss:\n",
    "            determinant = torch.det(X_batch)\n",
    "            inverse_batch = torch.linalg.inv(X_batch)\n",
    "            adjoin = determinant * inverse_batch\n",
    "            loss = loss_fn(adjoin @ predicted, adjoin @ y_batch)\n",
    "        else:\n",
    "            loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        loss_epoch += loss.detach()\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer_ft.zero_grad()\n",
    "        optimizer_adam.zero_grad()\n",
    "        # optimizer3.zero_grad()\n",
    "\n",
    "        # backpropagation (compute gradient)\n",
    "        loss.backward()\n",
    "\n",
    "        # update model parameters\n",
    "        # optimizer3.step()\n",
    "        optimizer_ft.step(det_batch=determinant, t=batch_num+1)\n",
    "        optimizer_adam.step()\n",
    "\n",
    "    # evaluate\n",
    "    mean_loss_test, mean_metric_test = validation_epoch(model, loss_fn, metric_fn, test_dataloader, \"cpu\")\n",
    "    loss_val.append(mean_loss_test)\n",
    "    score_val.append(mean_metric_test)\n",
    "\n",
    "    if torch.isnan(mean_metric_test):\n",
    "        print('can not apply')\n",
    "        break\n",
    "\n",
    "    print(f'on the {epoch}th epoch: loss = {(loss_epoch) / (len(train_dataloader)):.3f} & score = {mean_metric_test}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
