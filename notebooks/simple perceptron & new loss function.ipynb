{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Target\n",
    "Сравнение стандартных оптимизаторов (доступных в PyTorch) при обучении однослойного перцептрона с новой функцией потерь, использующей присоединенную матрицу"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "from custom_data import make_regression_data, RegressionDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## help function for train & evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def training_simple_perceptron(optimizer_fn, loss_fn, metric_fn,\n",
    "                               data_train, data_test, input_features=10,\n",
    "                               learning_rate=1e-3, current_device=device,\n",
    "                               max_epochs=1000, metric_constraint=0.95,\n",
    "                               with_addition=False):\n",
    "    \"\"\"Train 1-layer perceptron\n",
    "    Parameters:\n",
    "        optimizer_fn (function): optimizer for update parameters\n",
    "        loss_fn (function): loss function to measure model error\n",
    "        metric_fn (function): function to evaluate model accuracy\n",
    "        data_train (torch.utils.data.DataLoader): train dataloader\n",
    "        data_test (torch.utils.data.DataLoader): test dataloader\n",
    "        input_features (int): how many features our data has\n",
    "        learning_rate (float): speed for gradient descent\n",
    "        current_device (str): current available device (cuda or cpu)\n",
    "        max_epochs (int): the constraint for training epochs\n",
    "        metric_constraint (float): the lower constraint for score function\n",
    "        with_addition (bool): do we use new loss function or not (see picture below)\n",
    "    \"\"\"\n",
    "    simple_perceptron = nn.Linear(in_features=input_features, out_features=1, bias=False).to(device)\n",
    "    optimizer = optimizer_fn(params=simple_perceptron.parameters(),\n",
    "                             lr=learning_rate)\n",
    "    metric = 0.0\n",
    "    epoch = 1\n",
    "    while (metric < metric_constraint) and (epoch < max_epochs):\n",
    "        simple_perceptron.train()\n",
    "\n",
    "        for X_batch, y_batch in data_train:\n",
    "            X_batch, y_batch = X_batch.to(current_device), y_batch.to(current_device)\n",
    "            # forward pass\n",
    "            predicted = simple_perceptron(X_batch)\n",
    "\n",
    "            # loss computation\n",
    "            if with_addition:\n",
    "                determinant = torch.det(X_batch)\n",
    "                inverse_batch = torch.linalg.inv(X_batch)\n",
    "                adjoint = determinant * inverse_batch\n",
    "                loss = loss_fn(adjoint @ predicted, adjoint @ y_batch)\n",
    "            else:\n",
    "                loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "            # zero gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation (compute gradient)\n",
    "            loss.backward()\n",
    "\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # measure model quality\n",
    "        if epoch % 5 == 0:\n",
    "            #print(f'epoch {epoch}')\n",
    "            with torch.inference_mode():\n",
    "                metric, loss = 0.0, 0.0\n",
    "                for X_batch, y_batch in data_test:\n",
    "                    X_batch, y_batch = X_batch.to(current_device), y_batch.to(current_device)\n",
    "                    predicted = simple_perceptron(X_batch)\n",
    "                    loss += loss_fn(predicted, y_batch)\n",
    "                    metric += metric_fn(predicted, y_batch)\n",
    "                metric /= len(data_test)\n",
    "                loss /= len(data_test)\n",
    "                #print('on the test set: R2 score = {:.3f}, loss = {:.3f}'.format(metric, loss))\n",
    "        epoch += 1\n",
    "    return epoch, metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (torch.Size([80, 10]), torch.Size([80, 1]))\n",
      "shape of test: (torch.Size([20, 10]), torch.Size([20, 1]))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = make_regression_data(number_samples=100,\n",
    "                                                        number_features=10)\n",
    "print(f'shape of train: {X_train.shape, y_train.shape}\\nshape of test: {X_test.shape, y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of train sample:\n",
      " (tensor([-1.2478, -0.4400, -0.2526, -3.2413,  0.1307, -0.0595, -1.0244, -0.9269,\n",
      "         1.6324, -1.4301]), tensor([-501.1621]))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RegressionDataset(features=X_train,\n",
    "                                  labels=y_train)\n",
    "test_dataset = RegressionDataset(features=X_test,\n",
    "                                 labels=y_test)\n",
    "print(f'example of train sample:\\n {train_dataset[19]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch: features - torch.Size([10, 10]) and labels - torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=10\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "batch_example_features, batch_example_labels  = next(iter(train_dataloader))\n",
    "print('shape of batch: features - {} and labels - {}'.format(batch_example_features.shape, batch_example_labels.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers & loss function & metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "metric_fn = R2Score()\n",
    "\n",
    "optimizers = [optim.Adam, optim.SGD, optim.RMSprop, optim.AdamW, optim.Adamax]\n",
    "learning_rates = [1, 1e-3, 0.1, 1, 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with classic MSE loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "optimizers_comparison = dict()\n",
    "for optimizer, lr in zip(optimizers, learning_rates):\n",
    "    epochs, metric = training_simple_perceptron(optimizer_fn=optimizer,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                metric_fn=metric_fn,\n",
    "                                                data_train=train_dataloader,\n",
    "                                                data_test=test_dataloader,\n",
    "                                                learning_rate=lr)\n",
    "    optimizers_comparison[optimizer.__name__] = (epochs, metric)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam need 16 epochs for 0.975 score\n",
      "SGD need 91 epochs for 0.950 score\n",
      "RMSprop need 101 epochs for 0.953 score\n",
      "AdamW need 191 epochs for 0.950 score\n",
      "Adamax need 46 epochs for 0.967 score\n"
     ]
    }
   ],
   "source": [
    "for optim_name in optimizers_comparison:\n",
    "    epochs, metric = optimizers_comparison[optim_name]\n",
    "    print(f'{optim_name} need {epochs} epochs for {metric:.3f} score')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with non-classic MSE loss function\n",
    "![new loss](../new_loss.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "optimizers_comparison = dict()\n",
    "for optimizer, lr in zip(optimizers, learning_rates):\n",
    "    epochs, metric = training_simple_perceptron(optimizer_fn=optimizer,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                metric_fn=metric_fn,\n",
    "                                                data_train=train_dataloader,\n",
    "                                                data_test=test_dataloader,\n",
    "                                                learning_rate=lr,\n",
    "                                                with_addition=True)\n",
    "    optimizers_comparison[optimizer.__name__] = (epochs, metric)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam need 26 epochs for 0.950 score\n",
      "SGD need 6 epochs for nan score\n",
      "RMSprop need 306 epochs for 0.952 score\n",
      "AdamW need 1000 epochs for 0.281 score\n",
      "Adamax need 381 epochs for 0.954 score\n"
     ]
    }
   ],
   "source": [
    "for optim_name in optimizers_comparison:\n",
    "    epochs, metric = optimizers_comparison[optim_name]\n",
    "    print(f'{optim_name} need {epochs} epochs for {metric:.3f} score')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
